{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-31T01:53:26.527068Z","iopub.status.busy":"2024-05-31T01:53:26.526429Z","iopub.status.idle":"2024-05-31T01:53:30.844304Z","shell.execute_reply":"2024-05-31T01:53:30.843307Z","shell.execute_reply.started":"2024-05-31T01:53:26.527036Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn, optim\n","from tqdm import tqdm\n","import math\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","import json\n","import pandas as pd\n","import numpy as np\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T01:53:30.846827Z","iopub.status.busy":"2024-05-31T01:53:30.846215Z","iopub.status.idle":"2024-05-31T01:53:30.924163Z","shell.execute_reply":"2024-05-31T01:53:30.923260Z","shell.execute_reply.started":"2024-05-31T01:53:30.846798Z"},"trusted":true},"outputs":[],"source":["class ContinuousDiffusion(nn.Module):\n","    def __init__(self, model, noise_steps, beta_start, beta_end):\n","        super().__init__()\n","        self.noise_steps = noise_steps\n","        self.beta_start = beta_start\n","        self.beta_end = beta_end\n","\n","        self.beta = self.prepare_noise_schedule().to(device)\n","        self.alpha = 1. - self.beta\n","        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n","        self.model = model\n","        self.device = device\n","\n","    def prepare_noise_schedule(self):\n","        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n","    \n","    def directional_noise(self, x, mean, std_dev):\n","        eps = torch.randn_like(std_dev)\n","        bar_eps = mean + torch.multiply(std_dev, eps)\n","        eps_prime = torch.multiply(torch.sign(x), torch.abs(bar_eps))\n","        return eps_prime\n","\n","    def isotropic_noise(self):\n","        return torch.randn_like(self.std_dev)\n","\n","\n","    def make_noise(self, x, t):\n","        mean = torch.mean(x, dim=0)\n","        std_dev = torch.std(x, dim=0)\n","        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t]).to(device)\n","        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t]).to(device)\n","        noise = self.directional_noise(x, mean, std_dev).to(device)\n","    \n","        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * noise, noise\n","    \n","    def forward(self, x_0, context):\n","        t = torch.randint(low=1, high=self.noise_steps, size=(1,))\n","        x_t, noise = self.make_noise(x_0, t)\n","        pred_noise = self.model(torch.unsqueeze(x_t, dim=0), context)\n","        pred_noise = torch.squeeze(pred_noise)\n","        return noise, pred_noise, t, x_t\n","    \n","    def predict(self, x_0, x_t, eps, t):\n","        x_0_co = torch.sqrt(self.alpha_hat[t - 1]) * self.beta[t] / (1 - self.alpha_hat[t])\n","        x_0_co = x_0_co.to(device)\n","        x_t_co = torch.sqrt(self.alpha[t]) * (1 - self.alpha_hat[t - 1]) / (1 - self.alpha_hat[t])\n","        x_t_co = x_t_co.to(device)\n","        mean = x_0_co * x_0 + x_t_co * x_t\n","        std_dev = torch.sqrt(self.beta[t]).to(device)\n","        return mean * x_t + std_dev * eps\n","\n","class LightGCN(nn.Module):\n","    def __init__(self, num_users, num_items, latent_dim=64, n_layers=3) -> None:\n","        super(LightGCN, self).__init__()\n","\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.latent_dim = latent_dim\n","        self.n_layers = n_layers\n","        self.embedding_users = nn.Embedding(num_embeddings=self.num_users,\n","                                            embedding_dim=self.latent_dim)\n","        self.embedding_items = nn.Embedding(num_embeddings=self.num_items, \n","                                            embedding_dim=self.latent_dim)\n","        nn.init.xavier_uniform_(self.embedding_users.weight, gain=1)\n","        nn.init.xavier_uniform_(self.embedding_items.weight, gain=1)\n","\n","    def forward(self, pairs):\n","        row, col = pairs[0], pairs[1]\n","        index = torch.stack([row, col], dim=0)\n","        data = [1.0] * len(row)\n","        num_nodes = self.num_items + self.num_users\n","        graph = torch.sparse_coo_tensor(index, torch.tensor(data), size=(num_nodes, num_nodes))\n","\n","        users_emb = self.embedding_users.weight\n","        items_emb = self.embedding_items.weight\n","        all_emb = torch.cat([users_emb, items_emb])\n","        embs = [all_emb]\n","        \n","        for _ in range(self.n_layers):\n","            all_emb = all_emb.to(device)\n","            graph = graph.to(device)\n","            all_emb = torch.sparse.mm(graph, all_emb)\n","            embs.append(all_emb)\n","        \n","        embs = torch.stack(embs, dim=1)\n","        output_embs = torch.mean(embs, dim=1)\n","        \n","\n","        context = torch.zeros_like(output_embs)\n","        indices = torch.zeros(output_embs.shape[0])\n","        for i in range(len(row)):\n","            first = row[i]\n","            second = col[i]\n","            context[first] += output_embs[second]\n","            context[second] += output_embs[first]\n","            indices[first] += 1\n","            indices[second] += 1\n","        indices = indices.unsqueeze(1).expand(-1, output_embs.shape[1])\n","        context = context.to(device)\n","        indices = indices.to(device)\n","        context = context / indices\n","#         user, item = torch.split(output_embs, [self.num_users, self.num_items])\n","#         context_user, context_item = torch.split(context, [self.num_users, self.num_items])\n","        return output_embs, context\n","\n","def route_args(router, args, depth):\n","    routed_args = [(dict(), dict()) for _ in range(depth)]\n","    matched_keys = [key for key in args.keys() if key in router]\n","\n","    for key in matched_keys:\n","        val = args[key]\n","        for depth, ((f_args, g_args), routes) in enumerate(zip(routed_args, router[key])):\n","            new_f_args, new_g_args = map(lambda route: ({key: val} if route else {}), routes)\n","            routed_args[depth] = ({**f_args, **new_f_args}, {**g_args, **new_g_args})\n","    return routed_args\n","\n","def layer_drop(layers, prob):\n","    to_drop = torch.empty(len(layers)).uniform_(0, 1) < prob\n","    blocks = [block for block, drop in zip(layers, to_drop) if not drop]\n","    blocks = layers[:1] if len(blocks) == 0 else blocks\n","    return blocks\n","\n","def default(val, default_val):\n","    return val if val is not None else default_val\n","\n","def init_(tensor):\n","    dim = tensor.shape[-1]\n","    std = 1 / math.sqrt(dim)\n","    tensor.uniform_(-std, std)\n","    return tensor\n","\n","# helper classes\n","\n","class Residual(nn.Module):\n","    def __init__(self, fn):\n","        super().__init__()\n","        self.fn = fn\n","    def forward(self, x):\n","        return x + self.fn(x)\n","\n","class PreNorm(nn.Module):\n","    def __init__(self, dim, fn):\n","        super().__init__()\n","        self.fn = fn\n","        self.norm = nn.LayerNorm(dim)\n","    def forward(self, x):\n","        x = self.norm(x)\n","        return self.fn(x)\n","\n","class GELU_(nn.Module):\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n","\n","GELU = nn.GELU if hasattr(nn, 'GELU') else GELU_\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, mult = 4, dropout = 0., activation = None, glu = False):\n","        super().__init__()\n","        activation = default(activation, GELU)\n","\n","        self.glu = glu\n","        self.w1 = nn.Linear(dim, dim * mult * (2 if glu else 1))\n","        self.act = activation()\n","        self.dropout = nn.Dropout(dropout)\n","        self.w2 = nn.Linear(dim * mult, dim)\n","\n","    def forward(self, x, **kwargs):\n","        if not self.glu:\n","            x = self.w1(x)\n","            x = self.act(x)\n","        else:\n","            x, v = self.w1(x).chunk(2, dim=-1)\n","            x = self.act(x) * v\n","\n","        x = self.dropout(x)\n","        x = self.w2(x)\n","        return x\n","\n","class LinformerSelfAttention(nn.Module):\n","    def __init__(self, dim, seq_len, k = 256, heads = 8, dim_head = None, one_kv_head = False, share_kv = False, dropout = 0.):\n","        super().__init__()\n","        assert (dim % heads) == 0, 'dimension must be divisible by the number of heads'\n","\n","        self.seq_len = seq_len\n","        self.k = k\n","\n","        self.heads = heads\n","\n","        dim_head = default(dim_head, dim // heads)\n","        self.dim_head = dim_head\n","\n","        self.to_q = nn.Linear(dim, dim_head * heads, bias = False)\n","\n","        kv_dim = dim_head if one_kv_head else (dim_head * heads)\n","        self.to_k = nn.Linear(dim, kv_dim, bias = False)\n","        self.proj_k = nn.Parameter(init_(torch.zeros(seq_len, k)))\n","\n","        self.share_kv = share_kv\n","        if not share_kv:\n","            self.to_v = nn.Linear(dim, kv_dim, bias = False)\n","            self.proj_v = nn.Parameter(init_(torch.zeros(seq_len, k)))\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.to_out = nn.Linear(dim_head * heads, dim)\n","\n","    def forward(self, x, context = None, **kwargs):\n","        b, n, d, d_h, h, k = *x.shape, self.dim_head, self.heads, self.k\n","\n","        kv_len = n if context is None else context.shape[1]\n","        assert kv_len <= self.seq_len, f'the sequence length of the key / values must be {self.seq_len} - {kv_len} given'\n","\n","        queries = self.to_q(x)\n","\n","        proj_seq_len = lambda args: torch.einsum('bnd,nk->bkd', *args)\n","\n","        kv_input = x if context is None else context\n","\n","        keys = self.to_k(kv_input)\n","        values = self.to_v(kv_input) if not self.share_kv else keys\n","\n","        kv_projs = (self.proj_k, self.proj_v if not self.share_kv else self.proj_k)\n","\n","        # allow for variable sequence lengths (less than maximum sequence length) by slicing projections\n","\n","        if kv_len < self.seq_len:\n","            kv_projs = map(lambda t: t[:kv_len], kv_projs)\n","\n","        # project keys and values along the sequence length dimension to k\n","\n","        keys, values = map(proj_seq_len, zip((keys, values), kv_projs))\n","\n","        # merge head into batch for queries and key / values\n","\n","        queries = queries.reshape(b, n, h, -1).transpose(1, 2)\n","\n","        merge_key_values = lambda t: t.reshape(b, k, -1, d_h).transpose(1, 2).expand(-1, h, -1, -1)\n","        keys, values = map(merge_key_values, (keys, values))\n","\n","        # attention\n","\n","        dots = torch.einsum('bhnd,bhkd->bhnk', queries, keys) * (d_h ** -0.5)\n","        attn = dots.softmax(dim=-1)\n","        attn = self.dropout(attn)\n","        out = torch.einsum('bhnk,bhkd->bhnd', attn, values)\n","\n","        # split heads\n","        out = out.transpose(1, 2).reshape(b, n, -1)\n","        return self.to_out(out)\n","    \n","class SequentialSequence(nn.Module):\n","    def __init__(self, layers, args_route = {}, layer_dropout = 0.):\n","        super().__init__()\n","        assert all(len(route) == len(layers) for route in args_route.values()), 'each argument route map must have the same depth as the number of sequential layers'\n","        self.layers = layers\n","        self.args_route = args_route\n","        self.layer_dropout = layer_dropout\n","\n","    def forward(self, x, **kwargs):\n","        args = route_args(self.args_route, kwargs, len(self.layers))\n","        layers_and_args = list(zip(self.layers, args))\n","\n","        if self.training and self.layer_dropout > 0:\n","            layers_and_args = layer_drop(layers_and_args, self.layer_dropout)\n","\n","        for (f, g), (f_args, g_args) in layers_and_args:\n","            x = x + f(x, **f_args)\n","            x = x + g(x, **g_args)\n","        return x\n","\n","class Linformer(nn.Module):\n","    def __init__(self, dim, seq_len, depth, k = 256, heads = 8, dim_head = None, one_kv_head = False, share_kv = False, dropout = 0.):\n","        super().__init__()\n","        layers = nn.ModuleList([])\n","        for _ in range(depth):\n","            attn = LinformerSelfAttention(dim, seq_len, k = k, heads = heads, dim_head = dim_head, one_kv_head = one_kv_head, share_kv = share_kv, dropout = dropout)\n","            ff = FeedForward(dim, dropout = dropout)\n","\n","            layers.append(nn.ModuleList([\n","                PreNorm(dim, attn),\n","                PreNorm(dim, ff)\n","            ]))\n","\n","        self.net = SequentialSequence(layers)\n","\n","    def forward(self, x, context):\n","        return self.net(x, context=context)\n","    \n","\n","def cosine_similarity(emb_1, emb_2):\n","    emb_1 = np.array(emb_1)\n","    emb_2 = np.array(emb_2)\n","    return np.dot(emb_1, emb_2) / (np.linalg.norm(emb_1) * np.linalg.norm(emb_2))\n","\n","def topK(target, dictionary, top_k=5):\n","    list_dic = [(target, k) for k, v in sorted(dictionary.items(), key=lambda item: item[1], reverse=True)]\n","    return list_dic[:top_k]\n","\n","\n","\n","class TrainDataset(Dataset):\n","    def __init__(self, dataset_path, user_feature, item_feature, UI_train, n_enrich) -> None:\n","        super(TrainDataset, self).__init__()\n","\n","        self.UI_train = UI_train\n","        self.num_users = len(user_feature)\n","        self.num_items = len(item_feature)\n","\n","        augmented_pair = []\n","        if os.path.isfile(f'{dataset_path}/augmented_pair_{n_enrich}.csv'):\n","            import csv\n","            with open(f'{dataset_path}/augmented_pair_{n_enrich}.csv', 'r') as file:\n","                reader = csv.reader(file)\n","                data = list(reader)\n","            augmented_pair = [(int(row[0]), int(row[1])) for row in data]\n","\n","        else:\n","            for user_1 in user_feature:\n","                sim_dict = {}\n","                for user_2 in user_feature:\n","                    if user_1 != user_2:\n","                        sim_dict[user_2] = cosine_similarity(user_feature[user_1], user_feature[user_2])\n","\n","                top = topK(user_1, sim_dict, n_enrich)\n","                augmented_pair.extend(top)\n","\n","            for item_1 in item_feature:\n","                sim_dict = {}\n","                for item_2 in item_feature:\n","                    if item_1 != item_2:\n","                        sim_dict[item_2] = cosine_similarity(item_feature[item_1], item_feature[item_2])\n","                top = topK(item_1, sim_dict, n_enrich)\n","                augmented_pair.extend(top)\n","        \n","            augmented_pair.extend([(user, item) for user, item in zip(self.UI_train['user_ID'], self.UI_train['item_ID'])])\n","            augmented_pair.extend([(item, user) for user, item in zip(self.UI_train['user_ID'], self.UI_train['item_ID'])])\n","        self.augmented_pair = augmented_pair\n","\n","    def get_sparse_graph(self):\n","        data = [1.0] * len(self.augmented_pair)\n","        num_nodes = self.num_items + self.num_users\n","        return torch.sparse_coo_tensor(torch.tensor(self.augmented_pair).t(), torch.tensor(data), size=(num_nodes, num_nodes))\n","    \n","    def __getitem__(self, index):\n","        return self.augmented_pair[index]\n","    \n","    def __len__(self):\n","        return len(self.augmented_pair)\n","\n","class TestDataset(Dataset):\n","    def __init__(self, UI, num_users, num_items):\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.pairs = [(user, item) for user, item in zip(UI['user_ID'], UI['item_ID'])]\n","        self.pairs.extend([(item, user) for user, item in zip(UI['user_ID'], UI['item_ID'])])\n","\n","    def __getitem__(self, index):\n","        return self.pairs[index]\n","    \n","    def __len__(self):\n","        return len(self.pairs)\n","    \n","    def get_sparse_graph(self):\n","        data = [1.0] * len(self.pairs)\n","        num_nodes = self.num_items + self.num_users\n","        return torch.sparse_coo_tensor(torch.tensor(self.pairs).t(), torch.tensor(data), size=(num_nodes, num_nodes))\n","        \n","\n","\n","class Datasets:\n","    def __init__(self, dataset_path, batch_size, n_enrich) -> None:\n","        self.batch_size = batch_size\n","        item_feature = json.load(open(f'{dataset_path}/item_feature.json'))\n","        user_feature = json.load(open(f'{dataset_path}/user_feature.json'))\n","        self.UI_train = pd.read_csv(f'{dataset_path}/user_item_train.csv')\n","        self.UI_test = pd.read_csv(f'{dataset_path}/user_item_test.csv')\n","        self.UI_val = pd.read_csv(f'{dataset_path}/user_item_val.csv')\n","        self.num_users = len(user_feature)\n","        self.num_items = len(item_feature)\n","\n","        \n","        self.train_data = TrainDataset(dataset_path, user_feature, item_feature, self.UI_train, n_enrich)\n","        self.val_data = TestDataset(self.UI_val, self.num_users, self.num_items)\n","        self.test_data = TestDataset(self.UI_test, self.num_users, self.num_items)\n","        self.train_dataloader = DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True)\n","        self.val_dataloader = DataLoader(self.val_data, batch_size=len(self.val_data), shuffle=False)\n","        self.test_dataloader = DataLoader(self.test_data, batch_size=len(self.test_data), shuffle=False) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T01:53:30.925508Z","iopub.status.busy":"2024-05-31T01:53:30.925189Z","iopub.status.idle":"2024-05-31T01:53:30.940686Z","shell.execute_reply":"2024-05-31T01:53:30.939775Z","shell.execute_reply.started":"2024-05-31T01:53:30.925469Z"},"trusted":true},"outputs":[],"source":["def calculate_recall(prediction, ground_truth):\n","    total_true_positives = 0\n","    total_ground_truth = 0\n","    \n","    for user_idx, ground_truth_labels in ground_truth.items():\n","        user_prediction = prediction[user_idx]\n","        true_positives = 0\n","        \n","        for label in ground_truth_labels:\n","            if label in user_prediction:\n","                true_positives += 1\n","        \n","        total_true_positives += true_positives\n","        total_ground_truth += len(ground_truth_labels)\n","    \n","    recall = total_true_positives / total_ground_truth\n","    return recall\n","\n","def calculate_ndcg(prediction, ground_truth):\n","    total_ndcg = 0\n","    \n","    for user_idx, ground_truth_labels in ground_truth.items():\n","        user_prediction = prediction[user_idx]\n","        ideal_dcg = calculate_dcg(ground_truth_labels)\n","        actual_dcg = calculate_dcg([label for label in user_prediction if label in ground_truth_labels])\n","        \n","        if ideal_dcg == 0:\n","            ndcg = 1.0\n","        else:\n","            ndcg = actual_dcg / ideal_dcg\n","        \n","        total_ndcg += ndcg\n","    \n","    return total_ndcg / len(ground_truth)\n","import math\n","def calculate_dcg(labels):\n","    dcg = 0\n","    for i, label in enumerate(labels):\n","        dcg += label / math.log2(i + 2)\n","    return dcg"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T01:53:30.944289Z","iopub.status.busy":"2024-05-31T01:53:30.943712Z","iopub.status.idle":"2024-05-31T01:53:35.001320Z","shell.execute_reply":"2024-05-31T01:53:35.000489Z","shell.execute_reply.started":"2024-05-31T01:53:30.944235Z"},"trusted":true},"outputs":[],"source":["n_enrich = 10\n","dataset_path = '/kaggle/input/diffgt/datasets/foursquare'\n","dataset = Datasets(dataset_path=dataset_path, batch_size=2048, n_enrich=n_enrich)\n","\n","graph_encoder = LightGCN(dataset.num_users, dataset.num_items)\n","graph_encoder.train()\n","graph_encoder.to(device)\n","decoder = Linformer(dim=64, seq_len=dataset.num_users + dataset.num_items, depth=3)\n","decoder.train()\n","decoder.to(device)\n","dataloader = dataset.train_dataloader\n","diffusion = ContinuousDiffusion(decoder, noise_steps=100, beta_start=0, beta_end=1)\n","diffusion.to(device)\n","optimizer = optim.AdamW(decoder.parameters(), lr=1e-3)\n","diffusion_loss = nn.MSELoss()\n","\n","\n","def contrastive_loss(a, b, temp=0.2):\n","    infonce_criterion = nn.CrossEntropyLoss()\n","    a = nn.functional.normalize(a, dim=-1)\n","    b = nn.functional.normalize(b, dim=-1)\n","    logits = torch.mm(a, b.T)\n","    logits /= temp\n","    labels = torch.arange(a.shape[0]).to(a.device)\n","    return infonce_criterion(logits, labels)\n","import random\n","def bpr_loss(input, emb):\n","    t_user, t_item = input[0], input[1]\n","    pos_item = np.arange(dataset.num_users)\n","    neg_item = np.arange(dataset.num_users)\n","    for i in range(len(t_user)):\n","        user = t_user[i].item()\n","        item = t_item[i].item()\n","        if user > item: user, item = item, user\n","        if user < dataset.num_users and item >= dataset.num_users:\n","            pos_item[i] = item\n","    for i in range(len(t_user)):\n","        neg_item[i] = random.randint(dataset.num_users, dataset.num_users + dataset.num_items - 1)\n","    users_emb = emb[:dataset.num_users]\n","    pos_emb = emb[pos_item]\n","    neg_emb = emb[neg_item]\n","    pos_scores = torch.mul(users_emb, pos_emb)\n","    pos_scores = torch.sum(pos_scores, dim=0)\n","    neg_scores = torch.mul(users_emb, neg_emb)\n","    neg_scores = torch.sum(neg_scores, dim=0)\n","\n","    loss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n","    return loss\n","\n","def evaluate(dataloader, num_users, num_items, topK):\n","    graph_encoder.eval()\n","    decoder.eval()\n","    predict_items = []\n","    target_items = []\n","    \n","    with torch.no_grad():\n","        for batch_idx, input in enumerate(dataloader):\n","            x, context = graph_encoder(input)\n","            _, pred_noise, t, x_t = diffusion(x, context)\n","            emb = diffusion.predict(x, x_t, pred_noise, t)\n","            user_emb = emb[:num_users, :]\n","            item_emb = emb[num_users:, :]\n","            ranking = torch.matmul(user_emb, item_emb.t())\n","            _, predicted_indices = torch.topk(ranking, topK, dim=1)\n","            \n","            predicted_indices += num_users\n","            target_indices = {i: list() for i in range(num_users)}\n","            \n","            target_user, target_item = input[0], input[1]\n","            for i in range(len(target_user)):\n","                user = target_user[i].item()\n","                item = target_item[i].item()\n","                if user > item: user, item = item, user\n","                target_indices[user].append(item)\n","            recall = calculate_recall(predicted_indices, target_indices)\n","            NDCG = calculate_ndcg(predicted_indices, target_indices)\n","            print(f'Recall@{topK}: {recall}, NDCG@{topK}: {NDCG}')\n","    graph_encoder.train()\n","    decoder.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T01:53:47.506085Z","iopub.status.busy":"2024-05-31T01:53:47.505687Z","iopub.status.idle":"2024-05-31T01:58:27.538359Z","shell.execute_reply":"2024-05-31T01:58:27.537238Z","shell.execute_reply.started":"2024-05-31T01:53:47.506014Z"},"trusted":true},"outputs":[],"source":["import time\n","\n","for epoch in range(100):\n","    total_loss = 0\n","    for batch_idx, input in enumerate(tqdm(dataloader, total=len(dataloader))):\n","        start = time.time()\n","        x, context = graph_encoder(input)\n","        end_encoder = time.time()\n","        noise, pred_noise, t, x_t = diffusion(x, context)\n","        end_diffusion = time.time()\n","        emb = diffusion.predict(x, x_t, pred_noise, t)\n","        end_predict = time.time()\n","        print(\"LightGCN:\", end_encoder - start)\n","        print(\"Diffusion:\", end_diffusion - end_encoder)\n","        print(\"Prediction:\", end_predict - end_diffusion)\n","            \n","        loss = diffusion_loss(noise, pred_noise) + contrastive_loss(x, emb) + bpr_loss(input, emb)\n","        end_loss = time.time()\n","        print(\"Loss:\", end_loss - end_predict)\n","        total_loss += loss.item()\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss\n","       \n","    \n","    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tTotal loss: \", float(total_loss / batch_idx))\n","#     if (epoch) % 5 == 0:\n","    evaluate(dataset.val_dataloader, dataset.num_users, dataset.num_items, topK=20)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5100642,"sourceId":8538608,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
